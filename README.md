# Vision-Transformer-Model-from-Scratch-
 

This is a simple implementation of a Vision Transformer (ViT) for image classification. The model is built from scratch with basic components like patch embedding, positional encoding, and multi-head self-attention.  

It is trained and tested on the CIFAR-10 dataset and gives accuracy comparable to standard CNN models.  

 trained  and evaluate the model using the scripts provided. This project is meant to help understand how ViTs work under the hood without relying on libraries that abstract everything away.
